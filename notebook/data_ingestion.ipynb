{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66393490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e2cabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Learnings\\\\GenAI\\\\KNAC\\\\myfiles\\\\projects\\\\Agentic_Trading_Chatbot\\\\notebook'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeee7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8a71ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Learnings\\\\GenAI\\\\KNAC\\\\myfiles\\\\projects\\\\Agentic_Trading_Chatbot'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0c7f5",
   "metadata": {},
   "source": [
    "# contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38ede06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "from trading_bot.utils import load_yaml \n",
    "\n",
    "\n",
    "CONFIG=load_yaml(\"config/config.yaml\")\n",
    "\n",
    "@dataclass \n",
    "class DataIngestionConstants:\n",
    "    PINECONE_INDEX_NAME=CONFIG.PINECONE.INDEX_NAME\n",
    "    SPLITTER_CHUNK_SIZE=CONFIG.SPLITTER.CHUNK_SIZE\n",
    "    SPLITTER_CHUNK_OVERLAP_SIZE=CONFIG.SPLITTER.CHUNK_OVERLAP_SIZE\n",
    "    GOOGLE_EMBEDDING_MODEL_NAME=CONFIG.MODELS.EMBEDDINGS.GOOGLE\n",
    "    HUGGINGFACE_EMBEDDING_MODEL_NAME=CONFIG.MODELS.EMBEDDINGS.HUGGINGFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b216094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_INDEX_NAME: trading-bot\n",
      "SPLITTER_CHUNK_SIZE: 1000\n",
      "SPLITTER_CHUNK_OVERLAP_SIZE: 200\n",
      "GOOGLE_EMBEDDING_MODEL_NAME: models/text-embedding-004\n",
      "HUGGINGFACE_EMBEDDING_MODEL_NAME: BAAI/bge-small-en\n"
     ]
    }
   ],
   "source": [
    "print(\"PINECONE_INDEX_NAME:\", DataIngestionConstants.PINECONE_INDEX_NAME)\n",
    "print(\"SPLITTER_CHUNK_SIZE:\", DataIngestionConstants.SPLITTER_CHUNK_SIZE)\n",
    "print(\"SPLITTER_CHUNK_OVERLAP_SIZE:\", DataIngestionConstants.SPLITTER_CHUNK_OVERLAP_SIZE)\n",
    "print(\"GOOGLE_EMBEDDING_MODEL_NAME:\", DataIngestionConstants.GOOGLE_EMBEDDING_MODEL_NAME)\n",
    "print(\"HUGGINGFACE_EMBEDDING_MODEL_NAME:\", DataIngestionConstants.HUGGINGFACE_EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea241a",
   "metadata": {},
   "source": [
    "# entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94bafacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class DataIngestion:\n",
    "    PINECONE_INDEX_NAME:str\n",
    "    SPLITTER_CHUNK_SIZE:int\n",
    "    SPLITTER_CHUNK_OVERLAP_SIZE:int\n",
    "    GOOGLE_EMBEDDING_MODEL_NAME:str\n",
    "    HUGGINGFACE_EMBEDDING_MODEL_NAME:str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24975a1e",
   "metadata": {},
   "source": [
    "# configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6550473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "\n",
    "\n",
    "@dataclass \n",
    "class DataIngestionConfig:\n",
    "    PINECONE_INDEX_NAME=str(DataIngestionConstants.PINECONE_INDEX_NAME)\n",
    "    SPLITTER_CHUNK_SIZE=int(DataIngestionConstants.SPLITTER_CHUNK_SIZE)\n",
    "    SPLITTER_CHUNK_OVERLAP_SIZE=int(DataIngestionConstants.SPLITTER_CHUNK_OVERLAP_SIZE)\n",
    "    GOOGLE_EMBEDDING_MODEL_NAME=str(DataIngestionConstants.GOOGLE_EMBEDDING_MODEL_NAME)\n",
    "    HUGGINGFACE_EMBEDDING_MODEL_NAME=str(DataIngestionConstants.HUGGINGFACE_EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de51c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_INDEX_NAME: trading-bot\n",
      "<class 'str'>\n",
      "SPLITTER_CHUNK_SIZE: 1000\n",
      "<class 'int'>\n",
      "SPLITTER_CHUNK_OVERLAP_SIZE: 200\n",
      "<class 'int'>\n",
      "GOOGLE_EMBEDDING_MODEL_NAME: models/text-embedding-004\n",
      "<class 'str'>\n",
      "HUGGINGFACE_EMBEDDING_MODEL_NAME: BAAI/bge-small-en\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"PINECONE_INDEX_NAME:\", DataIngestionConfig.PINECONE_INDEX_NAME)\n",
    "print(type(DataIngestionConfig.PINECONE_INDEX_NAME))\n",
    "print(\"SPLITTER_CHUNK_SIZE:\", DataIngestionConfig.SPLITTER_CHUNK_SIZE)\n",
    "print(type(DataIngestionConfig.SPLITTER_CHUNK_SIZE))\n",
    "print(\"SPLITTER_CHUNK_OVERLAP_SIZE:\", DataIngestionConfig.SPLITTER_CHUNK_OVERLAP_SIZE)\n",
    "print(type(DataIngestionConfig.SPLITTER_CHUNK_OVERLAP_SIZE))\n",
    "print(\"GOOGLE_EMBEDDING_MODEL_NAME:\", DataIngestionConfig.GOOGLE_EMBEDDING_MODEL_NAME)\n",
    "print(type(DataIngestionConfig.GOOGLE_EMBEDDING_MODEL_NAME))\n",
    "print(\"HUGGINGFACE_EMBEDDING_MODEL_NAME:\", DataIngestionConfig.HUGGINGFACE_EMBEDDING_MODEL_NAME)\n",
    "print(type(DataIngestionConfig.HUGGINGFACE_EMBEDDING_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff1a0f8",
   "metadata": {},
   "source": [
    "# components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from trading_bot.utils import get_vector_store, load_embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from trading_bot.exception import CustomException\n",
    "from langchain_core.documents import Document\n",
    "from trading_bot.logger import logging\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "from typing import List\n",
    "import sys, tempfile, os\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionComponents:\n",
    "    __data_ingestion_config:DataIngestion\n",
    "\n",
    "    def load_documents(self, uploaded_files) -> List[Document]:\n",
    "        \"\"\"converts uploaded_files into langchain document object\n",
    "\n",
    "        Args:\n",
    "            uploaded_files : files uploaded\n",
    "\n",
    "        Returns:\n",
    "            List[Document]: list of document object to insert into vector store\n",
    "\n",
    "        Note:\n",
    "            uploaded_files must have .read() attribute\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In load_documents\")\n",
    "\n",
    "            documents = []\n",
    "            for uploaded_file in uploaded_files:\n",
    "                file_name = uploaded_file.filename\n",
    "                file_ext = os.path.splitext(uploaded_file.filename)[1].lower()\n",
    "                suffix = file_ext if file_ext in [\".pdf\", \".docx\"] else \".tmp\"\n",
    "\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp_file:\n",
    "                    temp_file.write(uploaded_file.file.read())\n",
    "                    temp_path = temp_file.name\n",
    "                    logging.info(f\"created temp file for {{{file_name}}}\")\n",
    "\n",
    "                if file_ext == \".pdf\":\n",
    "                    loader = PyPDFLoader(temp_path)\n",
    "                    documents.extend(loader.load())\n",
    "                    logging.info(f\"loaded temp file of {{{file_name}}} with PyPDFLoader\")\n",
    "\n",
    "                elif file_ext == \".docx\":\n",
    "                    loader = Docx2txtLoader(temp_path)\n",
    "                    documents.extend(loader.load())\n",
    "                    logging.info(f\"loaded temp file of {{{file_name}}} with Docx2txtLoader\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Unsupported file type: {file_name}\")\n",
    "\n",
    "            logging.info(\"Out load_documents\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def store_in_vector_db(self, documents: List[Document]):\n",
    "        \"\"\"inserts data into vector store after performing chunking\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): list of langchain document object to insert in vector store\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"In store_in_vector_db\")\n",
    "            # load api keys\n",
    "            load_status=load_dotenv()\n",
    "            logging.info(f\"load_dotenv: {{{load_status}}}\")\n",
    "\n",
    "            # create chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.__data_ingestion_config.SPLITTER_CHUNK_SIZE,\n",
    "                chunk_overlap=self.__data_ingestion_config.SPLITTER_CHUNK_OVERLAP_SIZE,\n",
    "                length_function=len\n",
    "            )\n",
    "            documents = text_splitter.split_documents(documents)\n",
    "            logging.info(f\"chunking completed, len(documents):{{{len(documents)}}}\")\n",
    "\n",
    "            # load embeddig model\n",
    "            embeddings=load_embeddings(self.__data_ingestion_config.GOOGLE_EMBEDDING_MODEL_NAME, \n",
    "                                       self.__data_ingestion_config.HUGGINGFACE_EMBEDDING_MODEL_NAME)\n",
    "            try:\n",
    "                embeddings_model_name=embeddings.model\n",
    "            except:\n",
    "                embeddings_model_name=embeddings.model_name\n",
    "            logging.info(f\"loaded embeddings model {{{embeddings_model_name}}}\")\n",
    "\n",
    "            # connect with vector store\n",
    "            vector_store = get_vector_store(pinecone_api_key=os.getenv(\"PINECONE_API_KEY\"), \n",
    "                                            index_name= self.__data_ingestion_config.PINECONE_INDEX_NAME, \n",
    "                                            embeddings=embeddings)\n",
    "            logging.info(\"loaded vector store\")\n",
    "\n",
    "            # insert data into vector store\n",
    "            logging.info(\"data insertion initiated\")\n",
    "            uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "            vector_store.add_documents(documents=documents, ids=uuids)\n",
    "            logging.info(\"data insertion completed\")\n",
    "\n",
    "            logging.info(\"Out store_in_vector_db\")\n",
    "        except Exception as e:\n",
    "            logging.exception(e)\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def run_pipeline(self, uploaded_files) -> None:\n",
    "        documents = self.load_documents(uploaded_files)\n",
    "        if not documents:\n",
    "            logging.warning(\"No valid documents found.\")\n",
    "            return\n",
    "        self.store_in_vector_db(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4e6fb",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd215ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trading_bot.components.data_ingestion import DataIngestionComponents\n",
    "from trading_bot.configuration import DataIngestionConfig\n",
    "\n",
    "\n",
    "class DataIngestionPipeline:\n",
    "    \n",
    "    def run(self, uploaded_files):\n",
    "        # data ingestion\n",
    "        self.data_ingestion=DataIngestionComponents(DataIngestionConfig)\n",
    "        self.run(uploaded_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a5d50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
