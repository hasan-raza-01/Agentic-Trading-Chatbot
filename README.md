# Agenticâ€‘Tradingâ€‘Chatbot: A RAGâ€‘Powered Trading Chatbot

An end-to-end, agentic Retrieval-Augmented Generation (RAG) system designed to deliver data-driven trading insights and automated execution. By combining customer-provided documents, real-time market data, and modular AI agents, this chatbot empowers traders with contextual, actionable intelligenceâ€”24/7, at scale.

---

## Hereâ€™s a preview of the appâ€™s user interface:
![UI Screenshot](./screenshots/ui-preview.png)

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ Dockerfile.backend        # Dockerfile for building the FastAPI backend
â”œâ”€â”€ Dockerfile.streamlit      # Dockerfile for building the Streamlit frontend
â”œâ”€â”€ ProjectConfig.json        # Project-specific configuration settings
â”œâ”€â”€ README.md                 # Project documentation and usage guide
â”œâ”€â”€ app.py                    # Entry point for the Streamlit Frontend Application
â”œâ”€â”€ config/                   # Directory for configuration files
â”œâ”€â”€ docker-compose.yml        # Docker Compose file for orchestrating backend and ui services
â”œâ”€â”€ main.py                   # Entry point for the FastAPI Backend Application
â”œâ”€â”€ notebook/                 # Jupyter notebooks for exploration and testing
â”œâ”€â”€ requirements.txt          # List of Python dependencies
â”œâ”€â”€ screenshots/              # Directory for storing images used in README
â”‚   â””â”€â”€ ui-preview.png        # Screenshot of the user interface
â”œâ”€â”€ setup.py                  # Script for installing the project as a package
â”œâ”€â”€ src/
â”‚   â””â”€â”€ trading_bot/          # Main package directory
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ components/       # Core business logic and components
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ agents.py     # Definitions of LangChain agents and workflow graph
â”‚       â”‚   â”œâ”€â”€ data_ingestion.py # Logic for parsing and processing uploaded documents
â”‚       â”‚   â””â”€â”€ tools.py      # Implementations of tools (Tavily, Polygon, Retriever)
â”‚       â”œâ”€â”€ configuration/    # Configuration management classes
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ constants/        # Project-wide constants
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ entity/           # Data entity definitions
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ exception/        # Custom exception handling logic
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger/           # Logging configuration and setup
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ pipeline/         # High-level orchestration pipelines
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ data_ingestion_pipeline/ # Pipeline for handling document ingestion
â”‚       â”‚   â”‚   â””â”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ query_pipeline/          # Pipeline for handling user queries and RAG
â”‚       â”‚       â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ schema/           # Pydantic models for request/response and state schemas
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â””â”€â”€ utils/            # Helper functions (I/O, Model loading, etc.)
â”‚           â””â”€â”€ __init__.py
â””â”€â”€ uv.lock                   # Lock file for ensuring reproducible dependency installs
```

---

## ğŸ”§ Core Workflow

1. **Document Ingestion & Processing**
   Uploads PDFs and DOCX stock market reports (e.g., research notes, earnings transcripts). Documents are parsed, chunked, and preprocessed into text segments.

2. **Vectorization & Indexing**
   Encodes text chunks into embeddings using LangChain and indexes them into Pinecone for low-latency semantic retrieval.

3. **Market Data Retrieval**
   Fetches live market pricing, historical data, and financial indicators via Polygon and Tavily APIs, grounding analysis in up-to-the-minute data.

4. **Agentic Reasoning & Execution**
   Orchestrates multiple AI agentsâ€”research, risk assessment, and order executionâ€”through LangChain Agents. Each agent collaborates to generate insights or place simulated trades.

5. **Real-Time Chat API**
   Exposes REST and WebSocket endpoints via FastAPI, running on Uvicorn for sub-200â€¯ms response times under load. Users interact through a Streamlit frontend or API clients.

---

## âœ… Key Capabilities

* **Document-Grounded Trading Insights**
  Answers grounded in your proprietary market research and uploaded documentsâ€”strategy analysis, risk factors, earnings summaries, and more.
* **Live Market Data Integration**
  Seamlessly integrates Polygon and Tavily to enrich context with real-time and historical market data.
* **Multi-Agent Automation**
  Modular agents handle distinct tasks (e.g., data retrieval, signal generation, order simulation), enabling complex, coordinated workflows.
* **Production-Ready Best Practices**
  Dockerized services, health checks, structured logging, and metrics (via Prometheus/Grafana) ensure reliability in production environments.
* **Extensible AI Stack**
  Swap LLM providers (Hugging Face, Google GenAI, Groq) or vector backends (Pinecone, AstraDB) with minimal code changes.

---

## ğŸš€ Deployment & CI/CD

* **GitHub Workflows**
  Automated lint, test, and build pipelines for every commit (`.github/workflows/main.yml`).
* **Docker Hub / AWS ECR**
  Hosts versioned Docker images for both backend (`Dockerfile.backend`) and UI (`Dockerfile.streamlit`).
* **Docker Compose**
  Orchestrates local development with `docker-compose.yml`, defining `backend` and `ui` services, health checks, and inter-service networking.
* **Environment-Driven Configuration**
  Manage secrets and endpoints via a `.env` file:

  ```bash
  POLYGON_API_KEY=your_polygon_key
  TAVILY_TOKEN=your_tavily_token
  PINECONE_API_KEY=your_pinecone_key
  GROQ_API_KEY=your_groq_key
  GOOGLE_API_KEY=your_google_key
  HF_TOKEN=your_huggingface_key
  ```

---

## ğŸƒ Running Locally

1. **Clone the repository**

   ```bash
   git clone https://github.com/hasan-raza-01/Agentic-Trading-Chatbot.git
   cd Agentic-Trading-Chatbot
   ```

2. **Set up environment variables**

   ```bash
   cp .env.example .env
   # Edit .env with your API keys and endpoints
   ```

3. **Create Virtual environment & Install Python dependencies**

   ```bash
   pip install --upgrade pip uv
   uv venv 
   .venv\scripts\activate
   uv pip install -e .
   ```

4. **Run FastAPI backend**

   ```bash
   uvicorn main:app --host 0.0.0.0 --port 8000 --reload
   ```

5. **Run Streamlit UI**

   ```bash
   streamlit run app.py
   ```

6. **(Alternative) Docker Compose**

   ```bash
   docker-compose up --build
   ```
